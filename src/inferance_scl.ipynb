{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv(\"../data/train_preprocessed.csv\")\n",
    "data.drop(columns=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X = data.drop(columns=[\"대출등급\"])  # 입력 변수\n",
    "y = data[\"대출등급\"]  # 타겟 변수\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# TabNet 모델 초기화\n",
    "model = TabNetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.54498 | val_0_accuracy: 0.4277  |  0:00:04s\n",
      "epoch 1  | loss: 1.08643 | val_0_accuracy: 0.53565 |  0:00:09s\n",
      "epoch 2  | loss: 0.92717 | val_0_accuracy: 0.54842 |  0:00:14s\n",
      "epoch 3  | loss: 0.82034 | val_0_accuracy: 0.60325 |  0:00:19s\n",
      "epoch 4  | loss: 0.75281 | val_0_accuracy: 0.60538 |  0:00:24s\n",
      "epoch 5  | loss: 0.6981  | val_0_accuracy: 0.66192 |  0:00:29s\n",
      "epoch 6  | loss: 0.69134 | val_0_accuracy: 0.66369 |  0:00:33s\n",
      "epoch 7  | loss: 0.6714  | val_0_accuracy: 0.5897  |  0:00:38s\n",
      "epoch 8  | loss: 0.67036 | val_0_accuracy: 0.65694 |  0:00:43s\n",
      "epoch 9  | loss: 0.66407 | val_0_accuracy: 0.60652 |  0:00:48s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 6 and best_val_0_accuracy = 0.66369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train.values, y_train.values, eval_set=[(X_valid.values, y_valid.values)], max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.61514 |  0:00:03s\n",
      "epoch 1  | loss: 1.14862 |  0:00:07s\n",
      "epoch 2  | loss: 1.00922 |  0:00:10s\n",
      "epoch 3  | loss: 0.94394 |  0:00:14s\n",
      "epoch 4  | loss: 0.84452 |  0:00:17s\n",
      "epoch 5  | loss: 0.79718 |  0:00:21s\n",
      "epoch 6  | loss: 0.78841 |  0:00:25s\n",
      "epoch 7  | loss: 0.75361 |  0:00:28s\n",
      "epoch 8  | loss: 0.72085 |  0:00:32s\n",
      "epoch 9  | loss: 0.71549 |  0:00:36s\n",
      "epoch 10 | loss: 0.68508 |  0:00:39s\n",
      "epoch 11 | loss: 0.69123 |  0:00:43s\n",
      "epoch 12 | loss: 0.67102 |  0:00:46s\n",
      "epoch 13 | loss: 0.66582 |  0:00:50s\n",
      "epoch 14 | loss: 0.66629 |  0:00:53s\n",
      "epoch 15 | loss: 0.64905 |  0:00:57s\n",
      "epoch 16 | loss: 0.64165 |  0:01:00s\n",
      "epoch 17 | loss: 0.63867 |  0:01:04s\n",
      "epoch 18 | loss: 0.64035 |  0:01:08s\n",
      "epoch 19 | loss: 0.67724 |  0:01:11s\n",
      "epoch 20 | loss: 0.67946 |  0:01:15s\n",
      "epoch 21 | loss: 0.67118 |  0:01:18s\n",
      "epoch 22 | loss: 0.64816 |  0:01:22s\n",
      "epoch 23 | loss: 0.64179 |  0:01:25s\n",
      "epoch 24 | loss: 0.7148  |  0:01:29s\n",
      "epoch 25 | loss: 0.64008 |  0:01:33s\n",
      "epoch 26 | loss: 0.62285 |  0:01:36s\n",
      "epoch 27 | loss: 0.62238 |  0:01:40s\n",
      "epoch 28 | loss: 0.6301  |  0:01:43s\n",
      "epoch 29 | loss: 0.61997 |  0:01:47s\n",
      "epoch 30 | loss: 0.60525 |  0:01:50s\n",
      "epoch 31 | loss: 0.59906 |  0:01:54s\n",
      "epoch 32 | loss: 0.60095 |  0:01:57s\n",
      "epoch 33 | loss: 0.58946 |  0:02:01s\n",
      "epoch 34 | loss: 0.60527 |  0:02:04s\n",
      "epoch 35 | loss: 0.5927  |  0:02:08s\n",
      "epoch 36 | loss: 0.58913 |  0:02:12s\n",
      "epoch 37 | loss: 0.58823 |  0:02:15s\n",
      "epoch 38 | loss: 0.59814 |  0:02:19s\n",
      "epoch 39 | loss: 0.59104 |  0:02:22s\n",
      "epoch 40 | loss: 0.60525 |  0:02:26s\n",
      "epoch 41 | loss: 0.58703 |  0:02:30s\n",
      "epoch 42 | loss: 0.58439 |  0:02:33s\n",
      "epoch 43 | loss: 0.5893  |  0:02:37s\n",
      "epoch 44 | loss: 0.5845  |  0:02:40s\n",
      "epoch 45 | loss: 0.58329 |  0:02:44s\n",
      "epoch 46 | loss: 0.5822  |  0:02:47s\n",
      "epoch 47 | loss: 0.57611 |  0:02:51s\n",
      "epoch 48 | loss: 0.58332 |  0:02:54s\n",
      "epoch 49 | loss: 0.58158 |  0:02:58s\n",
      "epoch 50 | loss: 0.57859 |  0:03:01s\n",
      "epoch 51 | loss: 0.57819 |  0:03:05s\n",
      "epoch 52 | loss: 0.57551 |  0:03:08s\n",
      "epoch 53 | loss: 0.58159 |  0:03:12s\n",
      "epoch 54 | loss: 0.57689 |  0:03:16s\n",
      "epoch 55 | loss: 0.57257 |  0:03:19s\n",
      "epoch 56 | loss: 0.57001 |  0:03:23s\n",
      "epoch 57 | loss: 0.56565 |  0:03:26s\n",
      "epoch 58 | loss: 0.57391 |  0:03:30s\n",
      "epoch 59 | loss: 0.56606 |  0:03:33s\n",
      "epoch 60 | loss: 0.57434 |  0:03:37s\n",
      "epoch 61 | loss: 0.57286 |  0:03:40s\n",
      "epoch 62 | loss: 0.57294 |  0:03:44s\n",
      "epoch 63 | loss: 0.56739 |  0:03:47s\n",
      "epoch 64 | loss: 0.56169 |  0:03:50s\n",
      "epoch 65 | loss: 0.56298 |  0:03:54s\n",
      "epoch 66 | loss: 0.57569 |  0:03:57s\n",
      "epoch 67 | loss: 0.56399 |  0:04:01s\n",
      "epoch 68 | loss: 0.56651 |  0:04:04s\n",
      "epoch 69 | loss: 0.55861 |  0:04:08s\n",
      "epoch 70 | loss: 0.56643 |  0:04:11s\n",
      "epoch 71 | loss: 0.56251 |  0:04:15s\n",
      "epoch 72 | loss: 0.5607  |  0:04:18s\n",
      "epoch 73 | loss: 0.56335 |  0:04:22s\n",
      "epoch 74 | loss: 0.5711  |  0:04:25s\n",
      "epoch 75 | loss: 0.5576  |  0:04:29s\n",
      "epoch 76 | loss: 0.57281 |  0:04:32s\n",
      "epoch 77 | loss: 0.55981 |  0:04:35s\n",
      "epoch 78 | loss: 0.54913 |  0:04:39s\n",
      "epoch 79 | loss: 0.55793 |  0:04:42s\n",
      "epoch 80 | loss: 0.56173 |  0:04:46s\n",
      "epoch 81 | loss: 0.55834 |  0:04:49s\n",
      "epoch 82 | loss: 0.55625 |  0:04:53s\n",
      "epoch 83 | loss: 0.55429 |  0:04:56s\n",
      "epoch 84 | loss: 0.5533  |  0:05:00s\n",
      "epoch 85 | loss: 0.55179 |  0:05:04s\n",
      "epoch 86 | loss: 0.55238 |  0:05:07s\n",
      "epoch 87 | loss: 0.55278 |  0:05:10s\n",
      "epoch 88 | loss: 0.54875 |  0:05:14s\n",
      "epoch 89 | loss: 0.54792 |  0:05:17s\n",
      "epoch 90 | loss: 0.55425 |  0:05:21s\n",
      "epoch 91 | loss: 0.54488 |  0:05:25s\n",
      "epoch 92 | loss: 0.55578 |  0:05:28s\n",
      "epoch 93 | loss: 0.55243 |  0:05:32s\n",
      "epoch 94 | loss: 0.54861 |  0:05:35s\n",
      "epoch 95 | loss: 0.55538 |  0:05:39s\n",
      "epoch 96 | loss: 0.54185 |  0:05:42s\n",
      "epoch 97 | loss: 0.5386  |  0:05:46s\n",
      "epoch 98 | loss: 0.5648  |  0:05:49s\n",
      "epoch 99 | loss: 0.54367 |  0:05:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.62412 |  0:00:03s\n",
      "epoch 1  | loss: 1.14276 |  0:00:07s\n",
      "epoch 2  | loss: 0.94612 |  0:00:10s\n",
      "epoch 3  | loss: 0.82726 |  0:00:14s\n",
      "epoch 4  | loss: 0.79177 |  0:00:17s\n",
      "epoch 5  | loss: 0.75139 |  0:00:21s\n",
      "epoch 6  | loss: 0.69704 |  0:00:24s\n",
      "epoch 7  | loss: 0.68288 |  0:00:28s\n",
      "epoch 8  | loss: 0.67288 |  0:00:31s\n",
      "epoch 9  | loss: 0.6641  |  0:00:35s\n",
      "epoch 10 | loss: 0.64194 |  0:00:38s\n",
      "epoch 11 | loss: 0.64078 |  0:00:42s\n",
      "epoch 12 | loss: 0.63622 |  0:00:45s\n",
      "epoch 13 | loss: 0.63031 |  0:00:49s\n",
      "epoch 14 | loss: 0.62879 |  0:00:52s\n",
      "epoch 15 | loss: 0.61441 |  0:00:56s\n",
      "epoch 16 | loss: 0.59785 |  0:00:59s\n",
      "epoch 17 | loss: 0.59926 |  0:01:03s\n",
      "epoch 18 | loss: 0.61088 |  0:01:06s\n",
      "epoch 19 | loss: 0.59955 |  0:01:10s\n",
      "epoch 20 | loss: 0.5913  |  0:01:13s\n",
      "epoch 21 | loss: 0.59539 |  0:01:17s\n",
      "epoch 22 | loss: 0.58653 |  0:01:20s\n",
      "epoch 23 | loss: 0.5901  |  0:01:24s\n",
      "epoch 24 | loss: 0.58474 |  0:01:27s\n",
      "epoch 25 | loss: 0.59007 |  0:01:31s\n",
      "epoch 26 | loss: 0.58408 |  0:01:35s\n",
      "epoch 27 | loss: 0.57103 |  0:01:39s\n",
      "epoch 28 | loss: 0.61423 |  0:01:42s\n",
      "epoch 29 | loss: 0.57755 |  0:01:46s\n",
      "epoch 30 | loss: 0.56967 |  0:01:49s\n",
      "epoch 31 | loss: 0.58819 |  0:01:53s\n",
      "epoch 32 | loss: 0.5829  |  0:01:57s\n",
      "epoch 33 | loss: 0.57433 |  0:02:00s\n",
      "epoch 34 | loss: 0.58015 |  0:02:04s\n",
      "epoch 35 | loss: 0.57377 |  0:02:07s\n",
      "epoch 36 | loss: 0.57    |  0:02:11s\n",
      "epoch 37 | loss: 0.55969 |  0:02:14s\n",
      "epoch 38 | loss: 0.56714 |  0:02:18s\n",
      "epoch 39 | loss: 0.56337 |  0:02:22s\n",
      "epoch 40 | loss: 0.56426 |  0:02:25s\n",
      "epoch 41 | loss: 0.56201 |  0:02:29s\n",
      "epoch 42 | loss: 0.56317 |  0:02:32s\n",
      "epoch 43 | loss: 0.56615 |  0:02:36s\n",
      "epoch 44 | loss: 0.5619  |  0:02:40s\n",
      "epoch 45 | loss: 0.55699 |  0:02:43s\n",
      "epoch 46 | loss: 0.56344 |  0:02:47s\n",
      "epoch 47 | loss: 0.56298 |  0:02:50s\n",
      "epoch 48 | loss: 0.56055 |  0:02:54s\n",
      "epoch 49 | loss: 0.55195 |  0:02:57s\n",
      "epoch 50 | loss: 0.56228 |  0:03:01s\n",
      "epoch 51 | loss: 0.55638 |  0:03:04s\n",
      "epoch 52 | loss: 0.55748 |  0:03:08s\n",
      "epoch 53 | loss: 0.55281 |  0:03:11s\n",
      "epoch 54 | loss: 0.54968 |  0:03:15s\n",
      "epoch 55 | loss: 0.55589 |  0:03:19s\n",
      "epoch 56 | loss: 0.55612 |  0:03:22s\n",
      "epoch 57 | loss: 0.56181 |  0:03:26s\n",
      "epoch 58 | loss: 0.55694 |  0:03:30s\n",
      "epoch 59 | loss: 0.55552 |  0:03:33s\n",
      "epoch 60 | loss: 0.55318 |  0:03:37s\n",
      "epoch 61 | loss: 0.54681 |  0:03:40s\n",
      "epoch 62 | loss: 0.55017 |  0:03:44s\n",
      "epoch 63 | loss: 0.54921 |  0:03:47s\n",
      "epoch 64 | loss: 0.54567 |  0:03:51s\n",
      "epoch 65 | loss: 0.54556 |  0:03:54s\n",
      "epoch 66 | loss: 0.55485 |  0:03:57s\n",
      "epoch 67 | loss: 0.54941 |  0:04:01s\n",
      "epoch 68 | loss: 0.56343 |  0:04:04s\n",
      "epoch 69 | loss: 0.56805 |  0:04:08s\n",
      "epoch 70 | loss: 0.55352 |  0:04:11s\n",
      "epoch 71 | loss: 0.54729 |  0:04:15s\n",
      "epoch 72 | loss: 0.55028 |  0:04:18s\n",
      "epoch 73 | loss: 0.55653 |  0:04:22s\n",
      "epoch 74 | loss: 0.54534 |  0:04:25s\n",
      "epoch 75 | loss: 0.54447 |  0:04:29s\n",
      "epoch 76 | loss: 0.5419  |  0:04:32s\n",
      "epoch 77 | loss: 0.54648 |  0:04:35s\n",
      "epoch 78 | loss: 0.53885 |  0:04:39s\n",
      "epoch 79 | loss: 0.54109 |  0:04:42s\n",
      "epoch 80 | loss: 0.55109 |  0:04:46s\n",
      "epoch 81 | loss: 0.54026 |  0:04:49s\n",
      "epoch 82 | loss: 0.54369 |  0:04:53s\n",
      "epoch 83 | loss: 0.54246 |  0:04:56s\n",
      "epoch 84 | loss: 0.5457  |  0:05:00s\n",
      "epoch 85 | loss: 0.54608 |  0:05:03s\n",
      "epoch 86 | loss: 0.5376  |  0:05:06s\n",
      "epoch 87 | loss: 0.54579 |  0:05:10s\n",
      "epoch 88 | loss: 0.54116 |  0:05:14s\n",
      "epoch 89 | loss: 0.5474  |  0:05:17s\n",
      "epoch 90 | loss: 0.54967 |  0:05:21s\n",
      "epoch 91 | loss: 0.53976 |  0:05:24s\n",
      "epoch 92 | loss: 0.53844 |  0:05:28s\n",
      "epoch 93 | loss: 0.5359  |  0:05:31s\n",
      "epoch 94 | loss: 0.53398 |  0:05:35s\n",
      "epoch 95 | loss: 0.54294 |  0:05:39s\n",
      "epoch 96 | loss: 0.53905 |  0:05:42s\n",
      "epoch 97 | loss: 0.54546 |  0:05:46s\n",
      "epoch 98 | loss: 0.53902 |  0:05:49s\n",
      "epoch 99 | loss: 0.54242 |  0:05:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.61642 |  0:00:03s\n",
      "epoch 1  | loss: 1.1008  |  0:00:07s\n",
      "epoch 2  | loss: 0.9573  |  0:00:10s\n",
      "epoch 3  | loss: 0.83641 |  0:00:14s\n",
      "epoch 4  | loss: 0.77112 |  0:00:17s\n",
      "epoch 5  | loss: 0.70242 |  0:00:20s\n",
      "epoch 6  | loss: 0.6653  |  0:00:24s\n",
      "epoch 7  | loss: 0.63968 |  0:00:27s\n",
      "epoch 8  | loss: 0.6351  |  0:00:31s\n",
      "epoch 9  | loss: 0.6334  |  0:00:34s\n",
      "epoch 10 | loss: 0.62015 |  0:00:38s\n",
      "epoch 11 | loss: 0.62126 |  0:00:42s\n",
      "epoch 12 | loss: 0.61586 |  0:00:45s\n",
      "epoch 13 | loss: 0.6088  |  0:00:49s\n",
      "epoch 14 | loss: 0.61745 |  0:00:52s\n",
      "epoch 15 | loss: 0.60577 |  0:00:56s\n",
      "epoch 16 | loss: 0.6011  |  0:00:59s\n",
      "epoch 17 | loss: 0.59434 |  0:01:03s\n",
      "epoch 18 | loss: 0.62842 |  0:01:06s\n",
      "epoch 19 | loss: 0.60261 |  0:01:10s\n",
      "epoch 20 | loss: 0.58566 |  0:01:13s\n",
      "epoch 21 | loss: 0.59121 |  0:01:16s\n",
      "epoch 22 | loss: 0.58858 |  0:01:20s\n",
      "epoch 23 | loss: 0.57984 |  0:01:24s\n",
      "epoch 24 | loss: 0.57774 |  0:01:27s\n",
      "epoch 25 | loss: 0.58858 |  0:01:30s\n",
      "epoch 26 | loss: 0.57264 |  0:01:34s\n",
      "epoch 27 | loss: 0.57436 |  0:01:37s\n",
      "epoch 28 | loss: 0.57744 |  0:01:41s\n",
      "epoch 29 | loss: 0.57791 |  0:01:44s\n",
      "epoch 30 | loss: 0.56857 |  0:01:47s\n",
      "epoch 31 | loss: 0.56885 |  0:01:51s\n",
      "epoch 32 | loss: 0.57447 |  0:01:54s\n",
      "epoch 33 | loss: 0.55811 |  0:01:58s\n",
      "epoch 34 | loss: 0.58978 |  0:02:01s\n",
      "epoch 35 | loss: 0.56944 |  0:02:05s\n",
      "epoch 36 | loss: 0.56762 |  0:02:08s\n",
      "epoch 37 | loss: 0.57415 |  0:02:12s\n",
      "epoch 38 | loss: 0.57028 |  0:02:15s\n",
      "epoch 39 | loss: 0.56017 |  0:02:18s\n",
      "epoch 40 | loss: 0.57167 |  0:02:22s\n",
      "epoch 41 | loss: 0.55691 |  0:02:25s\n",
      "epoch 42 | loss: 0.57044 |  0:02:29s\n",
      "epoch 43 | loss: 0.5727  |  0:02:32s\n",
      "epoch 44 | loss: 0.56736 |  0:02:36s\n",
      "epoch 45 | loss: 0.5501  |  0:02:39s\n",
      "epoch 46 | loss: 0.55333 |  0:02:42s\n",
      "epoch 47 | loss: 0.55636 |  0:02:46s\n",
      "epoch 48 | loss: 0.55505 |  0:02:49s\n",
      "epoch 49 | loss: 0.54551 |  0:02:53s\n",
      "epoch 50 | loss: 0.55668 |  0:02:56s\n",
      "epoch 51 | loss: 0.57045 |  0:03:00s\n",
      "epoch 52 | loss: 0.57764 |  0:03:03s\n",
      "epoch 53 | loss: 0.56387 |  0:03:07s\n",
      "epoch 54 | loss: 0.56159 |  0:03:10s\n",
      "epoch 55 | loss: 0.56654 |  0:03:14s\n",
      "epoch 56 | loss: 0.56499 |  0:03:17s\n",
      "epoch 57 | loss: 0.55522 |  0:03:21s\n",
      "epoch 58 | loss: 0.54865 |  0:03:24s\n",
      "epoch 59 | loss: 0.54905 |  0:03:27s\n",
      "epoch 60 | loss: 0.54119 |  0:03:31s\n",
      "epoch 61 | loss: 0.54149 |  0:03:34s\n",
      "epoch 62 | loss: 0.54305 |  0:03:38s\n",
      "epoch 63 | loss: 0.58859 |  0:03:41s\n",
      "epoch 64 | loss: 0.55106 |  0:03:45s\n",
      "epoch 65 | loss: 0.54783 |  0:03:48s\n",
      "epoch 66 | loss: 0.55163 |  0:03:51s\n",
      "epoch 67 | loss: 0.55483 |  0:03:55s\n",
      "epoch 68 | loss: 0.54405 |  0:03:58s\n",
      "epoch 69 | loss: 0.54903 |  0:04:02s\n",
      "epoch 70 | loss: 0.54535 |  0:04:05s\n",
      "epoch 71 | loss: 0.53863 |  0:04:09s\n",
      "epoch 72 | loss: 0.54512 |  0:04:12s\n",
      "epoch 73 | loss: 0.55897 |  0:04:16s\n",
      "epoch 74 | loss: 0.54474 |  0:04:19s\n",
      "epoch 75 | loss: 0.54561 |  0:04:23s\n",
      "epoch 76 | loss: 0.54023 |  0:04:26s\n",
      "epoch 77 | loss: 0.54607 |  0:04:30s\n",
      "epoch 78 | loss: 0.53628 |  0:04:34s\n",
      "epoch 79 | loss: 0.54415 |  0:04:37s\n",
      "epoch 80 | loss: 0.54586 |  0:04:41s\n",
      "epoch 81 | loss: 0.53653 |  0:04:45s\n",
      "epoch 82 | loss: 0.54163 |  0:04:48s\n",
      "epoch 83 | loss: 0.55033 |  0:04:52s\n",
      "epoch 84 | loss: 0.5439  |  0:04:56s\n",
      "epoch 85 | loss: 0.54583 |  0:04:59s\n",
      "epoch 86 | loss: 0.53554 |  0:05:03s\n",
      "epoch 87 | loss: 0.54847 |  0:05:06s\n",
      "epoch 88 | loss: 0.53243 |  0:05:09s\n",
      "epoch 89 | loss: 0.54244 |  0:05:13s\n",
      "epoch 90 | loss: 0.54182 |  0:05:16s\n",
      "epoch 91 | loss: 0.54288 |  0:05:20s\n",
      "epoch 92 | loss: 0.53714 |  0:05:23s\n",
      "epoch 93 | loss: 0.54149 |  0:05:27s\n",
      "epoch 94 | loss: 0.53446 |  0:05:30s\n",
      "epoch 95 | loss: 0.54164 |  0:05:34s\n",
      "epoch 96 | loss: 0.52914 |  0:05:37s\n",
      "epoch 97 | loss: 0.53944 |  0:05:40s\n",
      "epoch 98 | loss: 0.5302  |  0:05:44s\n",
      "epoch 99 | loss: 0.53926 |  0:05:47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.66734 |  0:00:03s\n",
      "epoch 1  | loss: 1.23629 |  0:00:06s\n",
      "epoch 2  | loss: 0.96212 |  0:00:10s\n",
      "epoch 3  | loss: 0.85203 |  0:00:13s\n",
      "epoch 4  | loss: 0.79188 |  0:00:17s\n",
      "epoch 5  | loss: 0.74689 |  0:00:20s\n",
      "epoch 6  | loss: 0.69426 |  0:00:24s\n",
      "epoch 7  | loss: 0.699   |  0:00:27s\n",
      "epoch 8  | loss: 0.68586 |  0:00:31s\n",
      "epoch 9  | loss: 0.65239 |  0:00:34s\n",
      "epoch 10 | loss: 0.66444 |  0:00:38s\n",
      "epoch 11 | loss: 0.67853 |  0:00:41s\n",
      "epoch 12 | loss: 0.65355 |  0:00:44s\n",
      "epoch 13 | loss: 0.63878 |  0:00:48s\n",
      "epoch 14 | loss: 0.6175  |  0:00:52s\n",
      "epoch 15 | loss: 0.62384 |  0:00:55s\n",
      "epoch 16 | loss: 0.61966 |  0:00:58s\n",
      "epoch 17 | loss: 0.62399 |  0:01:02s\n",
      "epoch 18 | loss: 0.61426 |  0:01:05s\n",
      "epoch 19 | loss: 0.61538 |  0:01:09s\n",
      "epoch 20 | loss: 0.62929 |  0:01:13s\n",
      "epoch 21 | loss: 0.6064  |  0:01:16s\n",
      "epoch 22 | loss: 0.61652 |  0:01:20s\n",
      "epoch 23 | loss: 0.59549 |  0:01:23s\n",
      "epoch 24 | loss: 0.59939 |  0:01:27s\n",
      "epoch 25 | loss: 0.59506 |  0:01:30s\n",
      "epoch 26 | loss: 0.59245 |  0:01:34s\n",
      "epoch 27 | loss: 0.59736 |  0:01:37s\n",
      "epoch 28 | loss: 0.59263 |  0:01:41s\n",
      "epoch 29 | loss: 0.60381 |  0:01:44s\n",
      "epoch 30 | loss: 0.58792 |  0:01:48s\n",
      "epoch 31 | loss: 0.57833 |  0:01:51s\n",
      "epoch 32 | loss: 0.57957 |  0:01:55s\n",
      "epoch 33 | loss: 0.6027  |  0:01:59s\n",
      "epoch 34 | loss: 0.59212 |  0:02:02s\n",
      "epoch 35 | loss: 0.59363 |  0:02:06s\n",
      "epoch 36 | loss: 0.57748 |  0:02:10s\n",
      "epoch 37 | loss: 0.58076 |  0:02:13s\n",
      "epoch 38 | loss: 0.5825  |  0:02:17s\n",
      "epoch 39 | loss: 0.58067 |  0:02:20s\n",
      "epoch 40 | loss: 0.5868  |  0:02:24s\n",
      "epoch 41 | loss: 0.56946 |  0:02:27s\n",
      "epoch 42 | loss: 0.5859  |  0:02:31s\n",
      "epoch 43 | loss: 0.5904  |  0:02:34s\n",
      "epoch 44 | loss: 0.57117 |  0:02:38s\n",
      "epoch 45 | loss: 0.56573 |  0:02:41s\n",
      "epoch 46 | loss: 0.5718  |  0:02:45s\n",
      "epoch 47 | loss: 0.57149 |  0:02:49s\n",
      "epoch 48 | loss: 0.57659 |  0:02:52s\n",
      "epoch 49 | loss: 0.58394 |  0:02:56s\n",
      "epoch 50 | loss: 0.57819 |  0:02:59s\n",
      "epoch 51 | loss: 0.55988 |  0:03:03s\n",
      "epoch 52 | loss: 0.59213 |  0:03:06s\n",
      "epoch 53 | loss: 0.58484 |  0:03:10s\n",
      "epoch 54 | loss: 0.60599 |  0:03:13s\n",
      "epoch 55 | loss: 0.57892 |  0:03:17s\n",
      "epoch 56 | loss: 0.62168 |  0:03:20s\n",
      "epoch 57 | loss: 0.57166 |  0:03:24s\n",
      "epoch 58 | loss: 0.56918 |  0:03:27s\n",
      "epoch 59 | loss: 0.5701  |  0:03:31s\n",
      "epoch 60 | loss: 0.56364 |  0:03:34s\n",
      "epoch 61 | loss: 0.55836 |  0:03:38s\n",
      "epoch 62 | loss: 0.56648 |  0:03:41s\n",
      "epoch 63 | loss: 0.56218 |  0:03:45s\n",
      "epoch 64 | loss: 0.55554 |  0:03:48s\n",
      "epoch 65 | loss: 0.57947 |  0:03:52s\n",
      "epoch 66 | loss: 0.57621 |  0:03:55s\n",
      "epoch 67 | loss: 0.5626  |  0:03:59s\n",
      "epoch 68 | loss: 0.55573 |  0:04:02s\n",
      "epoch 69 | loss: 0.55981 |  0:04:06s\n",
      "epoch 70 | loss: 0.55621 |  0:04:09s\n",
      "epoch 71 | loss: 0.55361 |  0:04:13s\n",
      "epoch 72 | loss: 0.5493  |  0:04:16s\n",
      "epoch 73 | loss: 0.54881 |  0:04:20s\n",
      "epoch 74 | loss: 0.5577  |  0:04:24s\n",
      "epoch 75 | loss: 0.54981 |  0:04:27s\n",
      "epoch 76 | loss: 0.56402 |  0:04:31s\n",
      "epoch 77 | loss: 0.56501 |  0:04:34s\n",
      "epoch 78 | loss: 0.54758 |  0:04:38s\n",
      "epoch 79 | loss: 0.54554 |  0:04:41s\n",
      "epoch 80 | loss: 0.55811 |  0:04:45s\n",
      "epoch 81 | loss: 0.55088 |  0:04:48s\n",
      "epoch 82 | loss: 0.54851 |  0:04:52s\n",
      "epoch 83 | loss: 0.55992 |  0:04:55s\n",
      "epoch 84 | loss: 0.54673 |  0:04:59s\n",
      "epoch 85 | loss: 0.55032 |  0:05:02s\n",
      "epoch 86 | loss: 0.55256 |  0:05:06s\n",
      "epoch 87 | loss: 0.56062 |  0:05:09s\n",
      "epoch 88 | loss: 0.54768 |  0:05:13s\n",
      "epoch 89 | loss: 0.55281 |  0:05:16s\n",
      "epoch 90 | loss: 0.54919 |  0:05:20s\n",
      "epoch 91 | loss: 0.55352 |  0:05:23s\n",
      "epoch 92 | loss: 0.54264 |  0:05:27s\n",
      "epoch 93 | loss: 0.54169 |  0:05:30s\n",
      "epoch 94 | loss: 0.56012 |  0:05:34s\n",
      "epoch 95 | loss: 0.54178 |  0:05:37s\n",
      "epoch 96 | loss: 0.55787 |  0:05:41s\n",
      "epoch 97 | loss: 0.56355 |  0:05:44s\n",
      "epoch 98 | loss: 0.55638 |  0:05:48s\n",
      "epoch 99 | loss: 0.54935 |  0:05:51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.62446 |  0:00:03s\n",
      "epoch 1  | loss: 1.13137 |  0:00:07s\n",
      "epoch 2  | loss: 1.00334 |  0:00:10s\n",
      "epoch 3  | loss: 0.92996 |  0:00:14s\n",
      "epoch 4  | loss: 0.85866 |  0:00:17s\n",
      "epoch 5  | loss: 0.8028  |  0:00:21s\n",
      "epoch 6  | loss: 0.71832 |  0:00:24s\n",
      "epoch 7  | loss: 0.71452 |  0:00:28s\n",
      "epoch 8  | loss: 0.69248 |  0:00:31s\n",
      "epoch 9  | loss: 0.67723 |  0:00:35s\n",
      "epoch 10 | loss: 0.66822 |  0:00:38s\n",
      "epoch 11 | loss: 0.65461 |  0:00:42s\n",
      "epoch 12 | loss: 0.66702 |  0:00:45s\n",
      "epoch 13 | loss: 0.66573 |  0:00:49s\n",
      "epoch 14 | loss: 0.65047 |  0:00:52s\n",
      "epoch 15 | loss: 0.65259 |  0:00:56s\n",
      "epoch 16 | loss: 0.64061 |  0:00:59s\n",
      "epoch 17 | loss: 0.64925 |  0:01:03s\n",
      "epoch 18 | loss: 0.6394  |  0:01:06s\n",
      "epoch 19 | loss: 0.62723 |  0:01:10s\n",
      "epoch 20 | loss: 0.63946 |  0:01:13s\n",
      "epoch 21 | loss: 0.61932 |  0:01:17s\n",
      "epoch 22 | loss: 0.61733 |  0:01:20s\n",
      "epoch 23 | loss: 0.61014 |  0:01:24s\n",
      "epoch 24 | loss: 0.63446 |  0:01:28s\n",
      "epoch 25 | loss: 0.60108 |  0:01:31s\n",
      "epoch 26 | loss: 0.60136 |  0:01:35s\n",
      "epoch 27 | loss: 0.61651 |  0:01:38s\n",
      "epoch 28 | loss: 0.60607 |  0:01:42s\n",
      "epoch 29 | loss: 0.63072 |  0:01:45s\n",
      "epoch 30 | loss: 0.62942 |  0:01:49s\n",
      "epoch 31 | loss: 0.61208 |  0:01:52s\n",
      "epoch 32 | loss: 0.59408 |  0:01:56s\n",
      "epoch 33 | loss: 0.60591 |  0:01:59s\n",
      "epoch 34 | loss: 0.59785 |  0:02:03s\n",
      "epoch 35 | loss: 0.6044  |  0:02:06s\n",
      "epoch 36 | loss: 0.57844 |  0:02:10s\n",
      "epoch 37 | loss: 0.5834  |  0:02:13s\n",
      "epoch 38 | loss: 0.61023 |  0:02:17s\n",
      "epoch 39 | loss: 0.60792 |  0:02:20s\n",
      "epoch 40 | loss: 0.59422 |  0:02:24s\n",
      "epoch 41 | loss: 0.5837  |  0:02:27s\n",
      "epoch 42 | loss: 0.58915 |  0:02:31s\n",
      "epoch 43 | loss: 0.59636 |  0:02:34s\n",
      "epoch 44 | loss: 0.5755  |  0:02:38s\n",
      "epoch 45 | loss: 0.57442 |  0:02:41s\n",
      "epoch 46 | loss: 0.59413 |  0:02:45s\n",
      "epoch 47 | loss: 0.58279 |  0:02:48s\n",
      "epoch 48 | loss: 0.58027 |  0:02:52s\n",
      "epoch 49 | loss: 0.5773  |  0:02:55s\n",
      "epoch 50 | loss: 0.5875  |  0:02:59s\n",
      "epoch 51 | loss: 0.57547 |  0:03:02s\n",
      "epoch 52 | loss: 0.60449 |  0:03:06s\n",
      "epoch 53 | loss: 0.56915 |  0:03:09s\n",
      "epoch 54 | loss: 0.58462 |  0:03:13s\n",
      "epoch 55 | loss: 0.59038 |  0:03:16s\n",
      "epoch 56 | loss: 0.58714 |  0:03:20s\n",
      "epoch 57 | loss: 0.58505 |  0:03:23s\n",
      "epoch 58 | loss: 0.57543 |  0:03:27s\n",
      "epoch 59 | loss: 0.56467 |  0:03:30s\n",
      "epoch 60 | loss: 0.56267 |  0:03:34s\n",
      "epoch 61 | loss: 0.58241 |  0:03:37s\n",
      "epoch 62 | loss: 0.5729  |  0:03:41s\n",
      "epoch 63 | loss: 0.56073 |  0:03:44s\n",
      "epoch 64 | loss: 0.57062 |  0:03:48s\n",
      "epoch 65 | loss: 0.57618 |  0:03:51s\n",
      "epoch 66 | loss: 0.57093 |  0:03:55s\n",
      "epoch 67 | loss: 0.56971 |  0:03:58s\n",
      "epoch 68 | loss: 0.57044 |  0:04:02s\n",
      "epoch 69 | loss: 0.56158 |  0:04:05s\n",
      "epoch 70 | loss: 0.56552 |  0:04:09s\n",
      "epoch 71 | loss: 0.56701 |  0:04:12s\n",
      "epoch 72 | loss: 0.57289 |  0:04:16s\n",
      "epoch 73 | loss: 0.56583 |  0:04:19s\n",
      "epoch 74 | loss: 0.5588  |  0:04:22s\n",
      "epoch 75 | loss: 0.55684 |  0:04:26s\n",
      "epoch 76 | loss: 0.55532 |  0:04:29s\n",
      "epoch 77 | loss: 0.56577 |  0:04:33s\n",
      "epoch 78 | loss: 0.56537 |  0:04:36s\n",
      "epoch 79 | loss: 0.55362 |  0:04:40s\n",
      "epoch 80 | loss: 0.56872 |  0:04:43s\n",
      "epoch 81 | loss: 0.55627 |  0:04:47s\n",
      "epoch 82 | loss: 0.56342 |  0:04:50s\n",
      "epoch 83 | loss: 0.55633 |  0:04:54s\n",
      "epoch 84 | loss: 0.55989 |  0:04:57s\n",
      "epoch 85 | loss: 0.55311 |  0:05:01s\n",
      "epoch 86 | loss: 0.54448 |  0:05:04s\n",
      "epoch 87 | loss: 0.55815 |  0:05:08s\n",
      "epoch 88 | loss: 0.56829 |  0:05:11s\n",
      "epoch 89 | loss: 0.55572 |  0:05:14s\n",
      "epoch 90 | loss: 0.55352 |  0:05:18s\n",
      "epoch 91 | loss: 0.5551  |  0:05:21s\n",
      "epoch 92 | loss: 0.54563 |  0:05:25s\n",
      "epoch 93 | loss: 0.55109 |  0:05:28s\n",
      "epoch 94 | loss: 0.54531 |  0:05:32s\n",
      "epoch 95 | loss: 0.54801 |  0:05:35s\n",
      "epoch 96 | loss: 0.56337 |  0:05:39s\n",
      "epoch 97 | loss: 0.56645 |  0:05:42s\n",
      "epoch 98 | loss: 0.54587 |  0:05:46s\n",
      "epoch 99 | loss: 0.54914 |  0:05:49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.50629 |  0:00:03s\n",
      "epoch 1  | loss: 1.0603  |  0:00:07s\n",
      "epoch 2  | loss: 0.86329 |  0:00:11s\n",
      "epoch 3  | loss: 0.75127 |  0:00:15s\n",
      "epoch 4  | loss: 0.74989 |  0:00:18s\n",
      "epoch 5  | loss: 0.66715 |  0:00:22s\n",
      "epoch 6  | loss: 0.67514 |  0:00:26s\n",
      "epoch 7  | loss: 0.67239 |  0:00:29s\n",
      "epoch 8  | loss: 0.64945 |  0:00:33s\n",
      "epoch 9  | loss: 0.67214 |  0:00:37s\n",
      "epoch 10 | loss: 0.66582 |  0:00:41s\n",
      "epoch 11 | loss: 0.6839  |  0:00:44s\n",
      "epoch 12 | loss: 0.63385 |  0:00:48s\n",
      "epoch 13 | loss: 0.65366 |  0:00:52s\n",
      "epoch 14 | loss: 0.6354  |  0:13:03s\n",
      "epoch 15 | loss: 0.61018 |  0:13:07s\n",
      "epoch 16 | loss: 0.61739 |  0:13:11s\n",
      "epoch 17 | loss: 0.63299 |  0:13:14s\n",
      "epoch 18 | loss: 0.6254  |  0:13:18s\n",
      "epoch 19 | loss: 0.64142 |  0:13:22s\n",
      "epoch 20 | loss: 0.61417 |  0:13:26s\n",
      "epoch 21 | loss: 0.62329 |  0:13:29s\n",
      "epoch 22 | loss: 0.6083  |  0:13:33s\n",
      "epoch 23 | loss: 0.61266 |  0:13:37s\n",
      "epoch 24 | loss: 0.59884 |  0:13:40s\n",
      "epoch 25 | loss: 0.60096 |  0:13:44s\n",
      "epoch 26 | loss: 0.60489 |  0:13:48s\n",
      "epoch 27 | loss: 0.6384  |  0:15:33s\n",
      "epoch 28 | loss: 0.6181  |  0:15:36s\n",
      "epoch 29 | loss: 0.62628 |  0:15:40s\n",
      "epoch 30 | loss: 0.62738 |  0:15:44s\n",
      "epoch 31 | loss: 0.60607 |  0:15:47s\n",
      "epoch 32 | loss: 0.62036 |  0:15:51s\n",
      "epoch 33 | loss: 0.60104 |  0:15:55s\n",
      "epoch 34 | loss: 0.67724 |  0:15:59s\n",
      "epoch 35 | loss: 0.60192 |  0:16:02s\n",
      "epoch 36 | loss: 0.6301  |  0:16:06s\n",
      "epoch 37 | loss: 0.61465 |  0:16:10s\n",
      "epoch 38 | loss: 0.61006 |  0:16:13s\n",
      "epoch 39 | loss: 0.60902 |  0:16:19s\n",
      "epoch 40 | loss: 0.61254 |  0:16:22s\n",
      "epoch 41 | loss: 0.62694 |  0:16:26s\n",
      "epoch 42 | loss: 0.59839 |  0:16:29s\n",
      "epoch 43 | loss: 0.60884 |  0:16:33s\n",
      "epoch 44 | loss: 0.60265 |  0:16:36s\n",
      "epoch 45 | loss: 0.58968 |  0:16:40s\n",
      "epoch 46 | loss: 0.61254 |  0:16:44s\n",
      "epoch 47 | loss: 0.59643 |  0:16:47s\n",
      "epoch 48 | loss: 0.58829 |  0:16:51s\n",
      "epoch 49 | loss: 0.58505 |  0:16:55s\n",
      "epoch 50 | loss: 0.58068 |  0:16:58s\n",
      "epoch 51 | loss: 0.58137 |  0:17:02s\n",
      "epoch 52 | loss: 0.59133 |  0:17:05s\n",
      "epoch 53 | loss: 0.57993 |  0:17:09s\n",
      "epoch 54 | loss: 0.57856 |  0:17:12s\n",
      "epoch 55 | loss: 0.57258 |  0:17:16s\n",
      "epoch 56 | loss: 0.56842 |  0:17:19s\n",
      "epoch 57 | loss: 0.57767 |  0:17:22s\n",
      "epoch 58 | loss: 0.58111 |  0:17:26s\n",
      "epoch 59 | loss: 0.58257 |  0:17:29s\n",
      "epoch 60 | loss: 0.58006 |  0:17:32s\n",
      "epoch 61 | loss: 0.58057 |  0:17:36s\n",
      "epoch 62 | loss: 0.57763 |  0:17:39s\n",
      "epoch 63 | loss: 0.557   |  0:17:43s\n",
      "epoch 64 | loss: 0.57832 |  0:17:46s\n",
      "epoch 65 | loss: 0.57202 |  0:17:49s\n",
      "epoch 66 | loss: 0.57219 |  0:17:53s\n",
      "epoch 67 | loss: 0.5642  |  0:17:56s\n",
      "epoch 68 | loss: 0.56333 |  0:18:00s\n",
      "epoch 69 | loss: 0.58148 |  0:18:03s\n",
      "epoch 70 | loss: 0.57946 |  0:18:06s\n",
      "epoch 71 | loss: 0.57708 |  0:18:10s\n",
      "epoch 72 | loss: 0.56604 |  0:18:13s\n",
      "epoch 73 | loss: 0.57214 |  0:18:17s\n",
      "epoch 74 | loss: 0.58685 |  0:18:20s\n",
      "epoch 75 | loss: 0.57116 |  0:18:24s\n",
      "epoch 76 | loss: 0.68197 |  0:18:27s\n",
      "epoch 77 | loss: 0.59274 |  0:18:31s\n",
      "epoch 78 | loss: 0.57277 |  0:18:34s\n",
      "epoch 79 | loss: 0.55344 |  0:18:38s\n",
      "epoch 80 | loss: 0.58663 |  0:18:41s\n",
      "epoch 81 | loss: 0.57155 |  0:18:45s\n",
      "epoch 82 | loss: 0.56568 |  0:18:48s\n",
      "epoch 83 | loss: 0.57236 |  0:18:52s\n",
      "epoch 84 | loss: 0.55268 |  0:18:55s\n",
      "epoch 85 | loss: 0.56618 |  0:18:59s\n",
      "epoch 86 | loss: 0.58152 |  0:19:02s\n",
      "epoch 87 | loss: 0.56368 |  0:19:06s\n",
      "epoch 88 | loss: 0.56421 |  0:19:09s\n",
      "epoch 89 | loss: 0.59592 |  0:19:13s\n",
      "epoch 90 | loss: 0.57153 |  0:19:16s\n",
      "epoch 91 | loss: 0.58082 |  0:19:20s\n",
      "epoch 92 | loss: 0.59468 |  0:19:23s\n",
      "epoch 93 | loss: 0.60154 |  0:19:27s\n",
      "epoch 94 | loss: 0.56972 |  0:19:31s\n",
      "epoch 95 | loss: 0.56342 |  0:19:34s\n",
      "epoch 96 | loss: 0.58117 |  0:19:38s\n",
      "epoch 97 | loss: 0.55914 |  0:19:41s\n",
      "epoch 98 | loss: 0.57304 |  0:19:45s\n",
      "epoch 99 | loss: 0.55615 |  0:19:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.45242 |  0:00:03s\n",
      "epoch 1  | loss: 1.04623 |  0:00:07s\n",
      "epoch 2  | loss: 0.87075 |  0:00:10s\n",
      "epoch 3  | loss: 0.76607 |  0:00:14s\n",
      "epoch 4  | loss: 0.7261  |  0:00:18s\n",
      "epoch 5  | loss: 0.71262 |  0:00:21s\n",
      "epoch 6  | loss: 0.69885 |  0:00:25s\n",
      "epoch 7  | loss: 0.66687 |  0:00:28s\n",
      "epoch 8  | loss: 0.67364 |  0:00:32s\n",
      "epoch 9  | loss: 0.6659  |  0:00:36s\n",
      "epoch 10 | loss: 0.64505 |  0:00:39s\n",
      "epoch 11 | loss: 0.634   |  0:00:43s\n",
      "epoch 12 | loss: 0.63942 |  0:00:46s\n",
      "epoch 13 | loss: 0.62678 |  0:00:50s\n",
      "epoch 14 | loss: 0.67313 |  0:00:54s\n",
      "epoch 15 | loss: 0.65568 |  0:00:57s\n",
      "epoch 16 | loss: 0.64663 |  0:01:01s\n",
      "epoch 17 | loss: 0.62816 |  0:01:04s\n",
      "epoch 18 | loss: 0.62401 |  0:01:08s\n",
      "epoch 19 | loss: 0.62545 |  0:01:12s\n",
      "epoch 20 | loss: 0.69019 |  0:01:15s\n",
      "epoch 21 | loss: 0.81384 |  0:01:19s\n",
      "epoch 22 | loss: 0.70418 |  0:01:22s\n",
      "epoch 23 | loss: 0.6681  |  0:01:26s\n",
      "epoch 24 | loss: 0.66748 |  0:01:29s\n",
      "epoch 25 | loss: 0.64309 |  0:01:33s\n",
      "epoch 26 | loss: 0.64865 |  0:01:37s\n",
      "epoch 27 | loss: 0.65051 |  0:01:40s\n",
      "epoch 28 | loss: 0.63469 |  0:01:44s\n",
      "epoch 29 | loss: 0.62125 |  0:01:47s\n",
      "epoch 30 | loss: 0.61733 |  0:01:51s\n",
      "epoch 31 | loss: 0.61057 |  0:01:55s\n",
      "epoch 32 | loss: 0.6066  |  0:01:59s\n",
      "epoch 33 | loss: 0.5933  |  0:02:03s\n",
      "epoch 34 | loss: 0.60504 |  0:02:06s\n",
      "epoch 35 | loss: 0.61371 |  0:02:10s\n",
      "epoch 36 | loss: 0.61436 |  0:02:13s\n",
      "epoch 37 | loss: 0.5826  |  0:02:17s\n",
      "epoch 38 | loss: 0.59477 |  0:02:21s\n",
      "epoch 39 | loss: 0.59281 |  0:02:24s\n",
      "epoch 40 | loss: 0.58301 |  0:07:37s\n",
      "epoch 41 | loss: 0.58757 |  0:07:41s\n",
      "epoch 42 | loss: 0.57883 |  0:07:44s\n",
      "epoch 43 | loss: 0.58258 |  0:07:48s\n",
      "epoch 44 | loss: 0.58023 |  0:07:52s\n",
      "epoch 45 | loss: 0.57413 |  0:07:56s\n",
      "epoch 46 | loss: 0.58091 |  0:07:59s\n",
      "epoch 47 | loss: 0.57543 |  0:08:03s\n",
      "epoch 48 | loss: 0.57409 |  0:08:07s\n",
      "epoch 49 | loss: 0.57542 |  0:08:10s\n",
      "epoch 50 | loss: 0.5737  |  0:08:14s\n",
      "epoch 51 | loss: 0.5747  |  0:08:18s\n",
      "epoch 52 | loss: 0.56393 |  0:08:22s\n",
      "epoch 53 | loss: 0.57181 |  0:08:25s\n",
      "epoch 54 | loss: 0.56405 |  0:08:29s\n",
      "epoch 55 | loss: 0.56666 |  0:08:33s\n",
      "epoch 56 | loss: 0.56287 |  0:08:36s\n",
      "epoch 57 | loss: 0.56811 |  0:08:40s\n",
      "epoch 58 | loss: 0.55537 |  0:08:44s\n",
      "epoch 59 | loss: 0.56233 |  0:08:48s\n",
      "epoch 60 | loss: 0.5636  |  0:08:51s\n",
      "epoch 61 | loss: 0.56584 |  0:08:55s\n",
      "epoch 62 | loss: 0.56851 |  0:08:59s\n",
      "epoch 63 | loss: 0.56199 |  0:09:03s\n",
      "epoch 64 | loss: 0.55197 |  0:09:06s\n",
      "epoch 65 | loss: 0.55776 |  0:09:10s\n",
      "epoch 66 | loss: 0.54858 |  0:09:14s\n",
      "epoch 67 | loss: 0.55804 |  0:09:17s\n",
      "epoch 68 | loss: 0.55127 |  0:09:21s\n",
      "epoch 69 | loss: 0.55449 |  0:09:25s\n",
      "epoch 70 | loss: 0.561   |  0:09:29s\n",
      "epoch 71 | loss: 0.57447 |  0:09:32s\n",
      "epoch 72 | loss: 0.55844 |  0:09:36s\n",
      "epoch 73 | loss: 0.56469 |  0:09:40s\n",
      "epoch 74 | loss: 0.5584  |  0:09:43s\n",
      "epoch 75 | loss: 0.54692 |  0:09:47s\n",
      "epoch 76 | loss: 0.54256 |  0:09:51s\n",
      "epoch 77 | loss: 0.55011 |  0:09:55s\n",
      "epoch 78 | loss: 0.5506  |  0:09:58s\n",
      "epoch 79 | loss: 0.54276 |  0:10:02s\n",
      "epoch 80 | loss: 0.54082 |  0:10:06s\n",
      "epoch 81 | loss: 0.54924 |  0:10:09s\n",
      "epoch 82 | loss: 0.54179 |  0:10:13s\n",
      "epoch 83 | loss: 0.54059 |  0:10:16s\n",
      "epoch 84 | loss: 0.5406  |  0:10:20s\n",
      "epoch 85 | loss: 0.54394 |  0:10:23s\n",
      "epoch 86 | loss: 0.5524  |  0:10:27s\n",
      "epoch 87 | loss: 0.54904 |  0:10:30s\n",
      "epoch 88 | loss: 0.54549 |  0:10:34s\n",
      "epoch 89 | loss: 0.55136 |  0:10:37s\n",
      "epoch 90 | loss: 0.54489 |  0:10:41s\n",
      "epoch 91 | loss: 0.54088 |  0:10:44s\n",
      "epoch 92 | loss: 0.53595 |  0:10:48s\n",
      "epoch 93 | loss: 0.54176 |  0:10:51s\n",
      "epoch 94 | loss: 0.53656 |  0:10:54s\n",
      "epoch 95 | loss: 0.53799 |  0:10:58s\n",
      "epoch 96 | loss: 0.53564 |  0:11:01s\n",
      "epoch 97 | loss: 0.53188 |  0:11:05s\n",
      "epoch 98 | loss: 0.54206 |  0:11:08s\n",
      "epoch 99 | loss: 0.53761 |  0:11:12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.49629 |  0:00:03s\n",
      "epoch 1  | loss: 1.15301 |  0:00:07s\n",
      "epoch 2  | loss: 0.9544  |  0:00:10s\n",
      "epoch 3  | loss: 0.88392 |  0:00:14s\n",
      "epoch 4  | loss: 0.81845 |  0:00:17s\n",
      "epoch 5  | loss: 0.73677 |  0:00:21s\n",
      "epoch 6  | loss: 0.70004 |  0:00:25s\n",
      "epoch 7  | loss: 0.68602 |  0:00:28s\n",
      "epoch 8  | loss: 0.65049 |  0:00:32s\n",
      "epoch 9  | loss: 0.6498  |  0:00:35s\n",
      "epoch 10 | loss: 0.64754 |  0:00:39s\n",
      "epoch 11 | loss: 0.64331 |  0:00:43s\n",
      "epoch 12 | loss: 0.62801 |  0:00:46s\n",
      "epoch 13 | loss: 0.61778 |  0:17:17s\n",
      "epoch 14 | loss: 0.62842 |  0:17:21s\n",
      "epoch 15 | loss: 0.61156 |  0:17:25s\n",
      "epoch 16 | loss: 0.61203 |  0:17:28s\n",
      "epoch 17 | loss: 0.60558 |  0:17:32s\n",
      "epoch 18 | loss: 0.60372 |  0:17:36s\n",
      "epoch 19 | loss: 0.61314 |  0:17:39s\n",
      "epoch 20 | loss: 0.59989 |  0:17:43s\n",
      "epoch 21 | loss: 0.59224 |  0:17:47s\n",
      "epoch 22 | loss: 0.57817 |  0:17:51s\n",
      "epoch 23 | loss: 0.6009  |  0:17:54s\n",
      "epoch 24 | loss: 0.58755 |  0:17:58s\n",
      "epoch 25 | loss: 0.59485 |  0:18:02s\n",
      "epoch 26 | loss: 0.59392 |  0:26:42s\n",
      "epoch 27 | loss: 0.5914  |  0:26:46s\n",
      "epoch 28 | loss: 0.58133 |  0:27:03s\n",
      "epoch 29 | loss: 0.57751 |  0:27:06s\n",
      "epoch 30 | loss: 0.57525 |  0:27:10s\n",
      "epoch 31 | loss: 0.58041 |  0:27:14s\n",
      "epoch 32 | loss: 0.56742 |  0:29:36s\n",
      "epoch 33 | loss: 0.57572 |  0:29:40s\n",
      "epoch 34 | loss: 0.56068 |  0:29:43s\n",
      "epoch 35 | loss: 0.57114 |  0:30:35s\n",
      "epoch 36 | loss: 0.567   |  0:30:38s\n",
      "epoch 37 | loss: 0.55254 |  0:30:42s\n",
      "epoch 38 | loss: 0.55463 |  0:30:45s\n",
      "epoch 39 | loss: 0.57041 |  0:30:49s\n",
      "epoch 40 | loss: 0.55194 |  0:31:19s\n",
      "epoch 41 | loss: 0.56532 |  0:31:22s\n",
      "epoch 42 | loss: 0.56446 |  0:31:44s\n",
      "epoch 43 | loss: 0.56585 |  0:31:48s\n",
      "epoch 44 | loss: 0.5611  |  0:31:51s\n",
      "epoch 45 | loss: 0.54151 |  0:31:55s\n",
      "epoch 46 | loss: 0.55723 |  0:31:59s\n",
      "epoch 47 | loss: 0.53932 |  0:32:02s\n",
      "epoch 48 | loss: 0.54456 |  0:32:06s\n",
      "epoch 49 | loss: 0.56355 |  0:32:10s\n",
      "epoch 50 | loss: 0.54425 |  0:32:20s\n",
      "epoch 51 | loss: 0.54238 |  0:32:24s\n",
      "epoch 52 | loss: 0.54857 |  0:32:27s\n",
      "epoch 53 | loss: 0.5565  |  0:32:31s\n",
      "epoch 54 | loss: 0.57258 |  0:33:43s\n",
      "epoch 55 | loss: 0.56391 |  0:33:47s\n",
      "epoch 56 | loss: 0.5518  |  0:34:38s\n",
      "epoch 57 | loss: 0.54897 |  0:34:42s\n",
      "epoch 58 | loss: 0.54895 |  0:34:45s\n",
      "epoch 59 | loss: 0.55383 |  0:35:10s\n",
      "epoch 60 | loss: 0.54467 |  0:35:14s\n",
      "epoch 61 | loss: 0.54139 |  0:35:17s\n",
      "epoch 62 | loss: 0.54336 |  0:35:21s\n",
      "epoch 63 | loss: 0.54057 |  0:35:45s\n",
      "epoch 64 | loss: 0.54388 |  0:35:48s\n",
      "epoch 65 | loss: 0.55003 |  0:35:51s\n",
      "epoch 66 | loss: 0.53588 |  0:35:55s\n",
      "epoch 67 | loss: 0.54046 |  0:36:21s\n",
      "epoch 68 | loss: 0.54724 |  0:36:24s\n",
      "epoch 69 | loss: 0.53713 |  0:36:28s\n",
      "epoch 70 | loss: 0.53865 |  0:36:32s\n",
      "epoch 71 | loss: 0.53658 |  0:52:15s\n",
      "epoch 72 | loss: 0.53627 |  1:08:54s\n",
      "epoch 73 | loss: 0.52882 |  1:25:33s\n",
      "epoch 74 | loss: 0.54102 |  1:25:37s\n",
      "epoch 75 | loss: 0.52896 |  1:30:47s\n",
      "epoch 76 | loss: 0.54793 |  1:30:50s\n",
      "epoch 77 | loss: 0.54097 |  1:48:39s\n",
      "epoch 78 | loss: 0.53683 |  1:48:42s\n",
      "epoch 79 | loss: 0.53037 |  2:04:55s\n",
      "epoch 80 | loss: 0.51361 |  2:21:45s\n",
      "epoch 81 | loss: 0.52445 |  2:31:45s\n",
      "epoch 82 | loss: 0.51464 |  2:31:49s\n",
      "epoch 83 | loss: 0.5338  |  2:47:53s\n",
      "epoch 84 | loss: 0.52486 |  2:47:57s\n",
      "epoch 85 | loss: 0.53632 |  3:05:10s\n",
      "epoch 86 | loss: 0.53721 |  3:21:19s\n",
      "epoch 87 | loss: 0.54156 |  3:32:46s\n",
      "epoch 88 | loss: 0.55042 |  3:32:50s\n",
      "epoch 89 | loss: 0.53233 |  3:49:57s\n",
      "epoch 90 | loss: 0.52887 |  3:50:01s\n",
      "epoch 91 | loss: 0.53104 |  4:07:04s\n",
      "epoch 92 | loss: 0.53695 |  4:22:08s\n",
      "epoch 93 | loss: 0.5378  |  4:33:47s\n",
      "epoch 94 | loss: 0.53834 |  4:33:51s\n",
      "epoch 95 | loss: 0.53179 |  4:49:32s\n",
      "epoch 96 | loss: 0.53402 |  4:49:36s\n",
      "epoch 97 | loss: 0.53193 |  5:04:37s\n",
      "epoch 98 | loss: 0.52016 |  5:22:32s\n",
      "epoch 99 | loss: 0.52386 |  5:34:49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.48517 |  0:00:03s\n",
      "epoch 1  | loss: 1.14557 |  0:00:06s\n",
      "epoch 2  | loss: 0.92487 |  0:02:50s\n",
      "epoch 3  | loss: 0.80929 |  0:02:54s\n",
      "epoch 4  | loss: 0.73126 |  0:10:27s\n",
      "epoch 5  | loss: 0.69843 |  0:10:31s\n",
      "epoch 6  | loss: 0.66357 |  0:10:34s\n",
      "epoch 7  | loss: 0.62541 |  0:10:38s\n",
      "epoch 8  | loss: 0.61141 |  0:10:41s\n",
      "epoch 9  | loss: 0.60113 |  0:10:45s\n",
      "epoch 10 | loss: 0.59417 |  0:10:49s\n",
      "epoch 11 | loss: 0.59938 |  0:10:52s\n",
      "epoch 12 | loss: 0.59638 |  0:10:56s\n",
      "epoch 13 | loss: 0.58447 |  0:11:00s\n",
      "epoch 14 | loss: 0.58067 |  0:11:04s\n",
      "epoch 15 | loss: 0.57901 |  0:11:07s\n",
      "epoch 16 | loss: 0.58289 |  0:11:11s\n",
      "epoch 17 | loss: 0.57997 |  0:11:15s\n",
      "epoch 18 | loss: 0.61937 |  0:11:44s\n",
      "epoch 19 | loss: 0.59382 |  0:11:48s\n",
      "epoch 20 | loss: 0.57749 |  0:11:57s\n",
      "epoch 21 | loss: 0.57415 |  0:12:00s\n",
      "epoch 22 | loss: 0.56897 |  0:12:04s\n",
      "epoch 23 | loss: 0.57729 |  0:12:08s\n",
      "epoch 24 | loss: 0.57502 |  0:12:11s\n",
      "epoch 25 | loss: 0.56943 |  0:12:15s\n",
      "epoch 26 | loss: 0.57061 |  0:12:19s\n",
      "epoch 27 | loss: 0.57176 |  0:12:48s\n",
      "epoch 28 | loss: 0.56506 |  0:12:51s\n",
      "epoch 29 | loss: 0.57194 |  0:12:55s\n",
      "epoch 30 | loss: 0.55608 |  0:13:24s\n",
      "epoch 31 | loss: 0.5522  |  0:13:27s\n",
      "epoch 32 | loss: 0.56218 |  0:13:47s\n",
      "epoch 33 | loss: 0.55635 |  0:13:51s\n",
      "epoch 34 | loss: 0.53982 |  0:13:55s\n",
      "epoch 35 | loss: 0.56525 |  0:13:58s\n",
      "epoch 36 | loss: 0.54961 |  0:30:13s\n",
      "epoch 37 | loss: 0.54556 |  0:46:11s\n",
      "epoch 38 | loss: 0.5551  |  0:46:15s\n",
      "epoch 39 | loss: 0.56162 |  1:01:01s\n",
      "epoch 40 | loss: 0.54933 |  1:01:05s\n",
      "epoch 41 | loss: 0.54148 |  1:01:09s\n",
      "epoch 42 | loss: 0.55336 |  1:16:53s\n",
      "epoch 43 | loss: 0.56153 |  1:16:56s\n",
      "epoch 44 | loss: 0.55187 |  1:17:00s\n",
      "epoch 45 | loss: 0.54209 |  1:17:04s\n",
      "epoch 46 | loss: 0.57152 |  1:17:07s\n",
      "epoch 47 | loss: 0.60407 |  1:17:11s\n",
      "epoch 48 | loss: 0.57742 |  1:25:34s\n",
      "epoch 49 | loss: 0.57983 |  1:25:38s\n",
      "epoch 50 | loss: 0.56729 |  1:25:41s\n",
      "epoch 51 | loss: 0.56364 |  1:26:12s\n",
      "epoch 52 | loss: 0.56064 |  1:26:15s\n",
      "epoch 53 | loss: 0.55697 |  1:26:19s\n",
      "epoch 54 | loss: 0.54983 |  1:26:22s\n",
      "epoch 55 | loss: 0.5707  |  1:26:26s\n",
      "epoch 56 | loss: 0.56017 |  1:28:01s\n",
      "epoch 57 | loss: 0.56966 |  1:28:04s\n",
      "epoch 58 | loss: 0.54575 |  1:28:08s\n",
      "epoch 59 | loss: 0.55368 |  1:28:11s\n",
      "epoch 60 | loss: 0.56356 |  1:28:15s\n",
      "epoch 61 | loss: 0.55973 |  1:30:44s\n",
      "epoch 62 | loss: 0.54596 |  1:30:48s\n",
      "epoch 63 | loss: 0.56686 |  1:30:51s\n",
      "epoch 64 | loss: 0.55317 |  1:33:34s\n",
      "epoch 65 | loss: 0.55936 |  1:33:38s\n",
      "epoch 66 | loss: 0.53915 |  1:33:41s\n",
      "epoch 67 | loss: 0.55733 |  1:34:31s\n",
      "epoch 68 | loss: 0.56285 |  1:34:35s\n",
      "epoch 69 | loss: 0.54954 |  1:34:38s\n",
      "epoch 70 | loss: 0.5521  |  1:34:42s\n",
      "epoch 71 | loss: 0.55032 |  1:42:07s\n",
      "epoch 72 | loss: 0.56665 |  1:42:11s\n",
      "epoch 73 | loss: 0.55048 |  1:42:39s\n",
      "epoch 74 | loss: 0.53968 |  1:42:42s\n",
      "epoch 75 | loss: 0.54622 |  1:42:46s\n",
      "epoch 76 | loss: 0.55044 |  1:52:38s\n",
      "epoch 77 | loss: 0.54486 |  1:52:42s\n",
      "epoch 78 | loss: 0.5413  |  1:52:45s\n",
      "epoch 79 | loss: 0.54466 |  2:02:03s\n",
      "epoch 80 | loss: 0.5379  |  2:02:06s\n",
      "epoch 81 | loss: 0.53591 |  2:02:10s\n",
      "epoch 82 | loss: 0.54766 |  2:18:01s\n",
      "epoch 83 | loss: 0.54509 |  2:33:05s\n",
      "epoch 84 | loss: 0.5379  |  2:33:09s\n",
      "epoch 85 | loss: 0.54192 |  2:33:13s\n",
      "epoch 86 | loss: 0.54319 |  2:51:08s\n",
      "epoch 87 | loss: 0.53939 |  2:59:20s\n",
      "epoch 88 | loss: 0.53826 |  2:59:24s\n",
      "epoch 89 | loss: 0.54536 |  2:59:28s\n",
      "epoch 90 | loss: 0.53578 |  3:03:00s\n",
      "epoch 91 | loss: 0.54345 |  3:03:04s\n",
      "epoch 92 | loss: 0.54292 |  3:03:08s\n",
      "epoch 93 | loss: 0.54975 |  3:03:11s\n",
      "epoch 94 | loss: 0.53937 |  3:03:15s\n",
      "epoch 95 | loss: 0.54309 |  3:03:19s\n",
      "epoch 96 | loss: 0.54111 |  3:03:23s\n",
      "epoch 97 | loss: 0.54113 |  3:03:27s\n",
      "epoch 98 | loss: 0.5311  |  3:08:54s\n",
      "epoch 99 | loss: 0.52828 |  3:08:58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.457   |  0:05:54s\n",
      "epoch 1  | loss: 1.03409 |  0:05:57s\n",
      "epoch 2  | loss: 0.83238 |  0:06:01s\n",
      "epoch 3  | loss: 0.75673 |  0:06:05s\n",
      "epoch 4  | loss: 0.69196 |  0:21:09s\n",
      "epoch 5  | loss: 0.65815 |  0:21:13s\n",
      "epoch 6  | loss: 0.64164 |  0:21:17s\n",
      "epoch 7  | loss: 0.65322 |  0:21:20s\n",
      "epoch 8  | loss: 0.63986 |  0:21:24s\n",
      "epoch 9  | loss: 0.62382 |  0:21:28s\n",
      "epoch 10 | loss: 0.61941 |  0:21:43s\n",
      "epoch 11 | loss: 0.61538 |  0:21:47s\n",
      "epoch 12 | loss: 0.62161 |  0:24:51s\n",
      "epoch 13 | loss: 0.61472 |  0:24:55s\n",
      "epoch 14 | loss: 0.60172 |  0:39:59s\n",
      "epoch 15 | loss: 0.59837 |  0:40:03s\n",
      "epoch 16 | loss: 0.60492 |  0:40:06s\n",
      "epoch 17 | loss: 0.63038 |  0:40:10s\n",
      "epoch 18 | loss: 0.60052 |  0:40:14s\n",
      "epoch 19 | loss: 0.60055 |  0:40:18s\n",
      "epoch 20 | loss: 0.60507 |  0:40:21s\n",
      "epoch 21 | loss: 0.59024 |  0:55:02s\n",
      "epoch 22 | loss: 0.5914  |  0:55:06s\n",
      "epoch 23 | loss: 0.59327 |  1:11:38s\n",
      "epoch 24 | loss: 0.57801 |  1:11:42s\n",
      "epoch 25 | loss: 0.57729 |  1:26:46s\n",
      "epoch 26 | loss: 0.59446 |  1:26:50s\n",
      "epoch 27 | loss: 0.59627 |  1:26:54s\n",
      "epoch 28 | loss: 0.57429 |  1:26:58s\n",
      "epoch 29 | loss: 0.57556 |  1:27:01s\n",
      "epoch 30 | loss: 0.57035 |  1:27:05s\n",
      "epoch 31 | loss: 0.56237 |  1:27:09s\n",
      "epoch 32 | loss: 0.5691  |  1:28:06s\n",
      "epoch 33 | loss: 0.60256 |  1:28:10s\n",
      "epoch 34 | loss: 0.5817  |  1:46:08s\n",
      "epoch 35 | loss: 0.57837 |  1:52:36s\n",
      "epoch 36 | loss: 0.55925 |  1:52:39s\n",
      "epoch 37 | loss: 0.56033 |  1:52:43s\n",
      "epoch 38 | loss: 0.57317 |  1:56:03s\n",
      "epoch 39 | loss: 0.5866  |  1:56:07s\n",
      "epoch 40 | loss: 0.56241 |  2:13:18s\n",
      "epoch 41 | loss: 0.55615 |  2:13:22s\n",
      "epoch 42 | loss: 0.57332 |  2:29:22s\n",
      "epoch 43 | loss: 0.55957 |  2:44:34s\n",
      "epoch 44 | loss: 0.56706 |  2:57:03s\n",
      "epoch 45 | loss: 0.55547 |  2:57:07s\n",
      "epoch 46 | loss: 0.56071 |  3:14:29s\n",
      "epoch 47 | loss: 0.54793 |  3:14:32s\n",
      "epoch 48 | loss: 0.55625 |  3:30:07s\n",
      "epoch 49 | loss: 0.54863 |  3:47:40s\n",
      "epoch 50 | loss: 0.55415 |  3:58:03s\n",
      "epoch 51 | loss: 0.55551 |  3:58:07s\n",
      "epoch 52 | loss: 0.53845 |  3:58:11s\n",
      "epoch 53 | loss: 0.54265 |  4:13:27s\n",
      "epoch 54 | loss: 0.5511  |  4:29:59s\n",
      "epoch 55 | loss: 0.59242 |  4:30:02s\n",
      "epoch 56 | loss: 0.58312 |  4:47:08s\n",
      "epoch 57 | loss: 0.5595  |  4:59:04s\n",
      "epoch 58 | loss: 0.54352 |  4:59:08s\n",
      "epoch 59 | loss: 0.55816 |  5:15:58s\n",
      "epoch 60 | loss: 0.5488  |  5:32:02s\n",
      "epoch 61 | loss: 0.54323 |  5:32:06s\n",
      "epoch 62 | loss: 0.56114 |  5:48:40s\n",
      "epoch 63 | loss: 0.59105 |  6:00:04s\n",
      "epoch 64 | loss: 0.56105 |  6:00:08s\n",
      "epoch 65 | loss: 0.54655 |  6:16:51s\n",
      "epoch 66 | loss: 0.55839 |  6:21:54s\n",
      "epoch 67 | loss: 0.54004 |  6:21:58s\n",
      "epoch 68 | loss: 0.57187 |  6:22:04s\n",
      "epoch 69 | loss: 0.56017 |  6:22:07s\n",
      "epoch 70 | loss: 0.65292 |  6:26:58s\n",
      "epoch 71 | loss: 0.55434 |  6:27:02s\n",
      "epoch 72 | loss: 0.55222 |  6:27:17s\n",
      "epoch 73 | loss: 0.54628 |  6:27:21s\n",
      "epoch 74 | loss: 0.54235 |  6:27:24s\n",
      "epoch 75 | loss: 0.53805 |  6:27:33s\n",
      "epoch 76 | loss: 0.54622 |  6:27:36s\n",
      "epoch 77 | loss: 0.53502 |  6:34:43s\n",
      "epoch 78 | loss: 0.53245 |  6:34:47s\n",
      "epoch 79 | loss: 0.53768 |  6:34:51s\n",
      "epoch 80 | loss: 0.53894 |  6:52:08s\n",
      "epoch 81 | loss: 0.52907 |  7:01:06s\n",
      "epoch 82 | loss: 0.54657 |  7:01:10s\n",
      "epoch 83 | loss: 0.54562 |  7:17:37s\n",
      "epoch 84 | loss: 0.54539 |  7:35:05s\n",
      "epoch 85 | loss: 0.56377 |  7:35:09s\n",
      "epoch 86 | loss: 0.55922 |  7:50:28s\n",
      "epoch 87 | loss: 0.53298 |  7:54:13s\n",
      "epoch 88 | loss: 0.53029 |  7:54:17s\n",
      "epoch 89 | loss: 0.53785 |  8:02:07s\n",
      "epoch 90 | loss: 0.54529 |  8:02:11s\n",
      "epoch 91 | loss: 0.52908 |  8:17:15s\n",
      "epoch 92 | loss: 0.54625 |  8:17:19s\n",
      "epoch 93 | loss: 0.52467 |  8:17:23s\n",
      "epoch 94 | loss: 0.52366 |  8:17:26s\n",
      "epoch 95 | loss: 0.53672 |  8:17:30s\n",
      "epoch 96 | loss: 0.5367  |  8:17:34s\n",
      "epoch 97 | loss: 0.52299 |  8:17:38s\n",
      "epoch 98 | loss: 0.5441  |  8:20:43s\n",
      "epoch 99 | loss: 0.52717 |  8:20:47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.46308 |  0:00:04s\n",
      "epoch 1  | loss: 1.06913 |  0:05:03s\n",
      "epoch 2  | loss: 0.8831  |  0:05:07s\n",
      "epoch 3  | loss: 0.78353 |  0:05:10s\n",
      "epoch 4  | loss: 0.74819 |  0:05:14s\n",
      "epoch 5  | loss: 0.7059  |  0:05:19s\n",
      "epoch 6  | loss: 0.67513 |  0:06:23s\n",
      "epoch 7  | loss: 0.64978 |  0:06:27s\n",
      "epoch 8  | loss: 0.6464  |  0:06:31s\n",
      "epoch 9  | loss: 0.63716 |  0:08:50s\n",
      "epoch 10 | loss: 0.62788 |  0:08:54s\n",
      "epoch 11 | loss: 0.61176 |  0:08:58s\n",
      "epoch 12 | loss: 0.61329 |  0:09:02s\n",
      "epoch 13 | loss: 0.59619 |  0:09:06s\n",
      "epoch 14 | loss: 0.58903 |  0:11:15s\n",
      "epoch 15 | loss: 0.58691 |  0:11:19s\n",
      "epoch 16 | loss: 0.58069 |  0:11:23s\n",
      "epoch 17 | loss: 0.58381 |  0:25:45s\n",
      "epoch 18 | loss: 0.59287 |  0:42:14s\n",
      "epoch 19 | loss: 0.59915 |  0:42:18s\n",
      "epoch 20 | loss: 0.58746 |  0:59:16s\n",
      "epoch 21 | loss: 0.57913 |  1:17:16s\n",
      "epoch 22 | loss: 0.56408 |  1:26:44s\n",
      "epoch 23 | loss: 0.5769  |  1:26:48s\n",
      "epoch 24 | loss: 0.57058 |  1:31:32s\n",
      "epoch 25 | loss: 0.55663 |  1:31:36s\n",
      "epoch 26 | loss: 0.56513 |  1:46:57s\n",
      "epoch 27 | loss: 0.57068 |  1:47:00s\n",
      "epoch 28 | loss: 0.56782 |  1:50:53s\n",
      "epoch 29 | loss: 0.56972 |  1:50:56s\n",
      "epoch 30 | loss: 0.55167 |  1:51:00s\n",
      "epoch 31 | loss: 0.55583 |  1:51:04s\n",
      "epoch 32 | loss: 0.55454 |  1:51:08s\n",
      "epoch 33 | loss: 0.55133 |  1:53:48s\n",
      "epoch 34 | loss: 0.55071 |  1:53:52s\n",
      "epoch 35 | loss: 0.55892 |  1:53:56s\n",
      "epoch 36 | loss: 0.54473 |  1:56:10s\n",
      "epoch 37 | loss: 0.55163 |  1:56:14s\n",
      "epoch 38 | loss: 0.57231 |  1:56:17s\n",
      "epoch 39 | loss: 0.53991 |  1:59:59s\n",
      "epoch 40 | loss: 0.54164 |  2:00:03s\n",
      "epoch 41 | loss: 0.53713 |  2:16:27s\n",
      "epoch 42 | loss: 0.54044 |  2:27:45s\n",
      "epoch 43 | loss: 0.5406  |  2:27:49s\n",
      "epoch 44 | loss: 0.54642 |  2:45:45s\n",
      "epoch 45 | loss: 0.54692 |  2:48:06s\n",
      "epoch 46 | loss: 0.53629 |  2:48:10s\n",
      "epoch 47 | loss: 0.53974 |  3:05:48s\n",
      "epoch 48 | loss: 0.53268 |  3:21:21s\n",
      "epoch 49 | loss: 0.54277 |  3:21:25s\n",
      "epoch 50 | loss: 0.53568 |  3:23:15s\n",
      "epoch 51 | loss: 0.54997 |  3:23:19s\n",
      "epoch 52 | loss: 0.55364 |  3:23:22s\n",
      "epoch 53 | loss: 0.53728 |  3:23:26s\n",
      "epoch 54 | loss: 0.53896 |  3:24:46s\n",
      "epoch 55 | loss: 0.53603 |  3:24:50s\n",
      "epoch 56 | loss: 0.5355  |  3:24:55s\n",
      "epoch 57 | loss: 0.52014 |  3:27:26s\n",
      "epoch 58 | loss: 0.52662 |  3:27:30s\n",
      "epoch 59 | loss: 0.52329 |  3:27:33s\n",
      "epoch 60 | loss: 0.53302 |  3:27:37s\n",
      "epoch 61 | loss: 0.53047 |  3:27:41s\n",
      "epoch 62 | loss: 0.52435 |  3:27:45s\n",
      "epoch 63 | loss: 0.52124 |  3:27:49s\n",
      "epoch 64 | loss: 0.538   |  3:27:53s\n",
      "epoch 65 | loss: 0.52701 |  3:29:32s\n",
      "epoch 66 | loss: 0.53288 |  3:29:36s\n",
      "epoch 67 | loss: 0.53006 |  3:29:40s\n",
      "epoch 68 | loss: 0.51689 |  3:30:30s\n",
      "epoch 69 | loss: 0.5374  |  3:30:33s\n",
      "epoch 70 | loss: 0.51784 |  3:30:37s\n",
      "epoch 71 | loss: 0.52014 |  3:45:58s\n",
      "epoch 72 | loss: 0.51853 |  4:01:53s\n",
      "epoch 73 | loss: 0.52159 |  4:01:57s\n",
      "epoch 74 | loss: 0.52572 |  4:18:10s\n",
      "epoch 75 | loss: 0.52205 |  4:28:47s\n",
      "epoch 76 | loss: 0.53061 |  4:28:51s\n",
      "epoch 77 | loss: 0.51187 |  4:45:30s\n",
      "epoch 78 | loss: 0.51626 |  5:01:09s\n",
      "epoch 79 | loss: 0.50951 |  5:16:36s\n",
      "epoch 80 | loss: 0.50761 |  5:29:45s\n",
      "epoch 81 | loss: 0.50475 |  5:29:49s\n",
      "epoch 82 | loss: 0.51944 |  5:46:45s\n",
      "epoch 83 | loss: 0.52064 |  6:03:26s\n",
      "epoch 84 | loss: 0.50569 |  6:03:30s\n",
      "epoch 85 | loss: 0.51007 |  6:05:37s\n",
      "epoch 86 | loss: 0.51272 |  6:10:18s\n",
      "epoch 87 | loss: 0.51299 |  6:10:22s\n",
      "epoch 88 | loss: 0.52056 |  6:26:50s\n",
      "epoch 89 | loss: 0.50525 |  6:26:54s\n",
      "epoch 90 | loss: 0.49844 |  6:26:58s\n",
      "epoch 91 | loss: 0.5001  |  6:28:49s\n",
      "epoch 92 | loss: 0.50128 |  6:28:53s\n",
      "epoch 93 | loss: 0.50143 |  6:29:53s\n",
      "epoch 94 | loss: 0.51537 |  6:29:57s\n",
      "epoch 95 | loss: 0.5734  |  6:30:49s\n",
      "epoch 96 | loss: 0.52785 |  6:32:38s\n",
      "epoch 97 | loss: 0.51973 |  6:32:42s\n",
      "epoch 98 | loss: 0.51821 |  6:32:46s\n",
      "epoch 99 | loss: 0.51681 |  6:32:50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.40179 |  0:00:04s\n",
      "epoch 1  | loss: 0.96889 |  0:05:41s\n",
      "epoch 2  | loss: 0.85086 |  0:05:45s\n",
      "epoch 3  | loss: 0.77955 |  0:05:49s\n",
      "epoch 4  | loss: 0.74196 |  0:05:53s\n",
      "epoch 5  | loss: 0.70943 |  0:05:57s\n",
      "epoch 6  | loss: 0.67039 |  0:06:01s\n",
      "epoch 7  | loss: 0.66232 |  0:06:04s\n",
      "epoch 8  | loss: 0.64814 |  0:06:08s\n",
      "epoch 9  | loss: 0.64878 |  0:06:12s\n",
      "epoch 10 | loss: 0.64356 |  0:06:16s\n",
      "epoch 11 | loss: 0.62182 |  0:06:20s\n",
      "epoch 12 | loss: 0.63702 |  0:06:24s\n",
      "epoch 13 | loss: 0.60951 |  0:06:28s\n",
      "epoch 14 | loss: 0.61385 |  0:06:32s\n",
      "epoch 15 | loss: 0.60016 |  0:06:36s\n",
      "epoch 16 | loss: 0.58964 |  0:06:40s\n",
      "epoch 17 | loss: 0.60415 |  0:06:43s\n",
      "epoch 18 | loss: 0.59835 |  0:06:47s\n",
      "epoch 19 | loss: 0.59616 |  0:06:51s\n",
      "epoch 20 | loss: 0.59116 |  0:06:55s\n",
      "epoch 21 | loss: 0.57261 |  0:06:59s\n",
      "epoch 22 | loss: 0.57521 |  0:07:03s\n",
      "epoch 23 | loss: 0.58217 |  0:07:07s\n",
      "epoch 24 | loss: 0.57128 |  0:07:11s\n",
      "epoch 25 | loss: 0.57    |  0:07:15s\n",
      "epoch 26 | loss: 0.56419 |  0:07:19s\n",
      "epoch 27 | loss: 0.57407 |  0:07:23s\n",
      "epoch 28 | loss: 0.58742 |  0:07:27s\n",
      "epoch 29 | loss: 0.58358 |  0:07:31s\n",
      "epoch 30 | loss: 0.55683 |  0:07:35s\n",
      "epoch 31 | loss: 0.56096 |  0:07:39s\n",
      "epoch 32 | loss: 0.55883 |  0:07:43s\n",
      "epoch 33 | loss: 0.5653  |  0:07:47s\n",
      "epoch 34 | loss: 0.56034 |  0:07:51s\n",
      "epoch 35 | loss: 0.55798 |  0:07:55s\n",
      "epoch 36 | loss: 0.54766 |  0:07:59s\n",
      "epoch 37 | loss: 0.56444 |  0:08:03s\n",
      "epoch 38 | loss: 0.55787 |  0:08:07s\n",
      "epoch 39 | loss: 0.55233 |  0:08:11s\n",
      "epoch 40 | loss: 0.54511 |  0:08:15s\n",
      "epoch 41 | loss: 0.55061 |  0:08:19s\n",
      "epoch 42 | loss: 0.53792 |  0:08:23s\n",
      "epoch 43 | loss: 0.54462 |  0:08:27s\n",
      "epoch 44 | loss: 0.53411 |  0:08:30s\n",
      "epoch 45 | loss: 0.5564  |  0:08:34s\n",
      "epoch 46 | loss: 0.54827 |  0:08:38s\n",
      "epoch 47 | loss: 0.5475  |  0:08:42s\n",
      "epoch 48 | loss: 0.53204 |  0:08:46s\n",
      "epoch 49 | loss: 0.53739 |  0:08:50s\n",
      "epoch 50 | loss: 0.53986 |  0:08:55s\n",
      "epoch 51 | loss: 0.5436  |  0:08:59s\n",
      "epoch 52 | loss: 0.53833 |  0:09:04s\n",
      "epoch 53 | loss: 0.5372  |  0:09:08s\n",
      "epoch 54 | loss: 0.525   |  0:09:12s\n",
      "epoch 55 | loss: 0.54823 |  0:09:16s\n",
      "epoch 56 | loss: 0.53708 |  0:09:20s\n",
      "epoch 57 | loss: 0.51727 |  0:09:24s\n",
      "epoch 58 | loss: 0.52772 |  0:09:28s\n",
      "epoch 59 | loss: 0.52635 |  0:09:32s\n",
      "epoch 60 | loss: 0.54533 |  0:09:36s\n",
      "epoch 61 | loss: 0.53283 |  0:09:40s\n",
      "epoch 62 | loss: 0.55381 |  0:09:44s\n",
      "epoch 63 | loss: 0.53869 |  0:09:48s\n",
      "epoch 64 | loss: 0.5383  |  0:09:53s\n",
      "epoch 65 | loss: 0.53634 |  0:09:57s\n",
      "epoch 66 | loss: 0.53346 |  0:10:01s\n",
      "epoch 67 | loss: 0.5301  |  0:10:05s\n",
      "epoch 68 | loss: 0.526   |  0:10:09s\n",
      "epoch 69 | loss: 0.54418 |  0:10:13s\n",
      "epoch 70 | loss: 0.52476 |  0:10:17s\n",
      "epoch 71 | loss: 0.5254  |  0:10:21s\n",
      "epoch 72 | loss: 0.5259  |  0:10:25s\n",
      "epoch 73 | loss: 0.51817 |  0:10:29s\n",
      "epoch 74 | loss: 0.53045 |  0:10:33s\n",
      "epoch 75 | loss: 0.53081 |  0:10:37s\n",
      "epoch 76 | loss: 0.51689 |  0:10:41s\n",
      "epoch 77 | loss: 0.52234 |  0:10:45s\n",
      "epoch 78 | loss: 0.5322  |  0:10:49s\n",
      "epoch 79 | loss: 0.51575 |  0:10:54s\n",
      "epoch 80 | loss: 0.51883 |  0:10:58s\n",
      "epoch 81 | loss: 0.52208 |  0:11:02s\n",
      "epoch 82 | loss: 0.52448 |  0:11:06s\n",
      "epoch 83 | loss: 0.51933 |  0:11:10s\n",
      "epoch 84 | loss: 0.52092 |  0:11:14s\n",
      "epoch 85 | loss: 0.51557 |  0:11:18s\n",
      "epoch 86 | loss: 0.51656 |  0:11:22s\n",
      "epoch 87 | loss: 0.51643 |  0:11:26s\n",
      "epoch 88 | loss: 0.5116  |  0:11:30s\n",
      "epoch 89 | loss: 0.51485 |  0:11:34s\n",
      "epoch 90 | loss: 0.51673 |  0:11:38s\n",
      "epoch 91 | loss: 0.51516 |  0:11:42s\n",
      "epoch 92 | loss: 0.51855 |  0:11:46s\n",
      "epoch 93 | loss: 0.51475 |  0:11:50s\n",
      "epoch 94 | loss: 0.52035 |  0:11:54s\n",
      "epoch 95 | loss: 0.53079 |  0:11:58s\n",
      "epoch 96 | loss: 0.52129 |  0:12:02s\n",
      "epoch 97 | loss: 0.51059 |  0:12:06s\n",
      "epoch 98 | loss: 0.52579 |  0:12:10s\n",
      "epoch 99 | loss: 0.51524 |  0:12:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/being/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.437   |  0:00:04s\n",
      "epoch 1  | loss: 1.06502 |  0:00:08s\n",
      "epoch 2  | loss: 0.90218 |  0:00:12s\n",
      "epoch 3  | loss: 0.7649  |  0:00:16s\n",
      "epoch 4  | loss: 0.71779 |  0:00:20s\n",
      "epoch 5  | loss: 0.70666 |  0:00:24s\n",
      "epoch 6  | loss: 0.67342 |  0:00:28s\n",
      "epoch 7  | loss: 0.64344 |  0:00:32s\n",
      "epoch 8  | loss: 0.63013 |  0:00:37s\n",
      "epoch 9  | loss: 0.65171 |  0:00:41s\n",
      "epoch 10 | loss: 0.6261  |  0:00:45s\n",
      "epoch 11 | loss: 0.60562 |  0:00:49s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m tabnet \u001b[38;5;241m=\u001b[39m TabNetClassifier()\n\u001b[1;32m     16\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(tabnet, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최적의 하이퍼파라미터 조합:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최적의 성능:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    493\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/abstract_model.py:527\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    525\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(output, y)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:616\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    615\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(x)\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:492\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    491\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 492\u001b[0m     steps_output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(steps_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multi_task:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:181\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    179\u001b[0m M_feature_level \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(M, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_attention_matrix)\n\u001b[1;32m    180\u001b[0m masked_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(M_feature_level, x)\n\u001b[0;32m--> 181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m d \u001b[38;5;241m=\u001b[39m ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_d])\n\u001b[1;32m    183\u001b[0m steps_output\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:737\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 737\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecifics(x)\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:774\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    772\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;241m0.5\u001b[39m])\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst:  \u001b[38;5;66;03m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[0;32m--> 774\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglu_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m     layers_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_glu)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:804\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    803\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m--> 804\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(x[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim], torch\u001b[38;5;241m.\u001b[39msigmoid(x[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim :]))\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_batch_size)), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x_) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_tabnet/tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_batch_size)), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/batchnorm.py:174\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m-> 1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_parameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1604\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[1;32m   1605\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Pandas DataFrame을 Numpy 배열로 변환\n",
    "X_train_array = X_train.values\n",
    "y_train_array = y_train.values\n",
    "\n",
    "param_grid = {\n",
    "    'n_d': [8, 16, 32],\n",
    "    'n_a': [8, 16, 32]\n",
    "}\n",
    "\n",
    "tabnet = TabNetClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(tabnet, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_array, y_train_array)\n",
    "\n",
    "print(\"최적의 하이퍼파라미터 조합:\", grid_search.best_params_)\n",
    "print(\"최적의 성능:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
